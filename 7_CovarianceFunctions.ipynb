{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae945924",
   "metadata": {},
   "source": [
    "## 4 Covariance Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdd5e0",
   "metadata": {},
   "source": [
    "A covariance function (kernel) defines the covariance of the GP random variables. Kernel and mean function together define a GP. For a kernel $K$ to be valid, it must be positive definite. That is, it satisfies the property that $y^TKy>0$ for all $y\\in R^N$ where $y$ is a $n$x1 vector. This implies that $K$ must be a symmetric matrix and that it must be invertible.\n",
    "- _Stationary_ covariance function is a function $x-x'$. It is invariant to translations in input space. Ex: squared exponential kernel.\n",
    "- _Isotropic_ covariance function if it is a function of $\\lvert x-x' \\rvert$. Ex: squared exponential kernel.\n",
    "- _Dot product_ covariance function if it depends only on $x$ and $x'$ through $x\\cdot x'$. Ex: $k(x,x') = \\sigma_0^2 + x\\cdot x'$ from linear regression with $N(0,1)$ priors on the coefficients of $x_d$ and a prior of $N(0,\\sigma_0^2)$ on the bias $1$. Another is the polynomail kernel $k(x,x') = (\\sigma_0^2+x\\cdot x')^p$. They're invariant to rotation of coordinates but not translations.\n",
    "- _Gram matrix, or covariance matrix, K_ whose entries are $K_{ij} = k(x_i,x_j)$. It is a positive semidefinite matrix.\n",
    "\n",
    "Multiplying two kernels $K_1$ and $K_2$ together performs elementwise multiplication resulting in a new kernel $K=K_1K_2$ that will only have an element with a high value if $K_1$ and $K_2$ have high valued elements. This can be interpreted as an AND operation. Likewise, addition of two kernels can be interpreted as an OR operation, since either $K_1$ or $K_2$ has to have a high value element for $K=K_1+K_2$ to have a high value element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113d744",
   "metadata": {},
   "source": [
    "For higher dimensional GPRs, covariance functions like the squared exponential can be parameterized in terms of hyperparameters:\n",
    "$$\n",
    "k(x,x') = \\sigma_f^2\\exp(-\\frac{1}{2}(x-x')^TM(x-x'))\n",
    "$$\n",
    "where $\\theta = (\\{M\\},\\sigma_f^2)^T$ is the vector containing all the hyperparameters, assuming no noise, and $\\{M\\}$, or $C$, is the collection of parameters in the symmetric matrix $M$.\n",
    "<br>\n",
    "Choices for $M$:\n",
    "$$\n",
    "M = l^2I\n",
    "$$\n",
    "$$\n",
    "M = \\text{diag}(l)^2\n",
    "$$\n",
    "$$\n",
    "M = \\Lambda\\Lambda^T + \\text{diag}(l)^{-2}\n",
    "$$\n",
    "where $\\Lambda$ is a $D$x$k$ matrix.\n",
    "<br><br>\n",
    "Using the second $M$, we have $l_1,\\ldots,l_D$ hyperparameters which are the _characteristic length-scales_. That is, on each axis in input space there are different values of the lengthscale, each uncorrelated. If they're correlated, then we have the third $M$.\n",
    "<br>\n",
    "To determine each $l$, we apply automatic relevance determination (ARD), since each $l$ determines how relevant an input is.\n",
    "<br>\n",
    "The third $M$ requires _factor analysis distance_ which measures the relevance of one or more lengthscales combined (includes correlation).\n",
    "\n",
    "<br><br><br>\n",
    "Sources used:\n",
    "* GP book page 106"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
