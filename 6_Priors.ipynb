{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae945924",
   "metadata": {},
   "source": [
    "## Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995dc46b",
   "metadata": {},
   "source": [
    "A prior is an unconditional probability of a parameter, the assumed probability distribution of the parameter before evidence is considered. In the Bayesian model, the posterior probability distribution is an update to the prior, given the information from the data.\n",
    "\n",
    "How to choose a prior? If we have some past information on the prior, we have an _informative_ prior. When no information is available, we have an _uninformative prior_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0519f9",
   "metadata": {},
   "source": [
    "---\n",
    "### Jeffreys prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db1b35",
   "metadata": {},
   "source": [
    "Like the uniform prior, the Jeffreys prior is also an uninformative prior. However, if we were to use a uniform prior for a parameter, we can create a valid new parameterization and the distribution is no long equiprobable. This shows that the uniform prior is uninformative in one paramterization, but becomes informative through change of variables. Unlike the uniform prior, we would use a Jeffreys prior as it is invariant (unchanged) to the parameterization (defining parameters) used to define the prior. That is, regardless of how we scale the parameter (change of coordinates), the relative probability distribution doesn't change.\n",
    "<br><br>\n",
    "If $p_{\\phi}(\\phi) = p_{\\theta}(\\theta)\\lvert \\frac{d\\theta}{d\\phi}\\rvert$, and if we can perform a change of variables, $p_\\theta(\\theta)$ is invariant under parameterization.\n",
    "<br><br>\n",
    "The Fisher information transforms under parameterization:\n",
    "$$\n",
    "I_\\phi(\\phi) = I_\\theta(\\theta)\\left(\\frac{d\\theta}{d\\phi} \\right)^2\n",
    "$$\n",
    "but not if we take the square root:\n",
    "$$\n",
    "\\sqrt{I_\\phi(\\phi)} = \\sqrt{I_\\theta(\\theta)}\\lvert\\frac{d\\theta}{d\\phi}\\rvert\n",
    "$$\n",
    "So, we define $p_\\theta(\\theta)\\propto \\sqrt{I_\\theta(\\theta)}$\n",
    "<br><br>\n",
    "The Fisher information measures the amount of information about an unkown parameter $\\theta$ from a random variable $X$ (expectation here is the integral over the values of $X$ while keeping $\\theta$ fixed):\n",
    "$$\n",
    "I(\\theta) = -E\\left[\\frac{d^2}{d\\theta^2}\\log f(X\\vert \\theta) \\vert \\theta\\right]\n",
    "$$\n",
    "<br>\n",
    "this tells us that where $f(X\\vert \\theta)$ is low, this is informative. In terms of the MLE, we solve for $\\frac{d}{d\\theta}\\log(L(\\theta\\vert X)) = 0$ such that $\\frac{d^2}{d\\theta^2}\\log(L(\\theta\\vert X))<0$. A \"peaked\" MLE corresponds to high fisher information.\n",
    "\n",
    "<br><br><br>\n",
    "Sources\n",
    "* https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture6.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eceb7f4",
   "metadata": {},
   "source": [
    "---\n",
    "### Inverse-gamma prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d08c5",
   "metadata": {},
   "source": [
    "**Lemma:** If $x_i\\vert \\mu,\\sigma^2 \\sim N(\\mu,\\sigma^2)$ and $\\sigma^2\\sim IG(\\alpha,\\beta)$, then\n",
    "$$\n",
    "\\sigma^2\\vert x_1,x_2,\\cdots,x_n \\sim IG\\left(\\alpha+n/2,\\beta+\\sum(x_i-\\mu)/2\\right)\n",
    "$$\n",
    "where the inverse gamma distribution is\n",
    "$$\n",
    "f(x\\vert \\alpha,\\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}(1/x)^{\\alpha+1}\\exp(-\\beta/x)\n",
    "$$\n",
    "<br><br>\n",
    "Why does this matter?\n",
    "<br>\n",
    "A popular kenerl in GPR is:\n",
    "$$\n",
    "\\kappa(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2l^2}\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)^T\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)\\right)\n",
    "$$\n",
    "\n",
    "The (marginal) likelihood, or evidence of the joint normal distribution defining a GPR with $0$ as the mean function is the following:\n",
    "$$\n",
    "p(y\\vert X,\\theta) = (2\\pi)^{\\frac{-n}{2}}\\lvert K_y\\rvert^{\\frac{-n}{2}}\\exp\\left[-\\frac{1}{2}y^TK_y^{-1}y\\right]\n",
    "$$\n",
    "Since we must specify the priors for the parameters of the covariance function, which typically inclues a variance parameter (like $1/l^2 > 0$), the inverse gamma prior is a convient choice because it ensures that the variance hyperparameter stays positive $>0$.\n",
    "\n",
    "<br><br><br>\n",
    "Sources:\n",
    "* https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
